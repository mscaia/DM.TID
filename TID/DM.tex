\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tikz}
\usepackage[margin=2cm]{geometry}
\usetikzlibrary{shapes, arrows.meta, positioning}
\usepackage[labelfont=bf, font=small]{caption}  % Pour personnaliser le titre de la figure


\lstset{
  language=R,
  basicstyle=\ttfamily\small,
  numbers=left,               % Numérotation des lignes
  numberstyle=\tiny\color{gray}, % Style de la numérotation
  keywordstyle=\color{blue},  % Couleur des mots-clés
  commentstyle=\color{green}, % Couleur des commentaires
  stringstyle=\color{red},    % Couleur des chaînes de caractères
  breaklines=true,            % Coupe les lignes trop longues
  frame=single,               % Encadre le code
}


\title{TP1 TID}
\author{SCAIA Matteo, MARIAC Damien}
\date{\today} 

\begin{document}

\maketitle

\begin{figure}[h] 
    \centering
    \includegraphics[width=0.5\textwidth]{ssd_logo.png} 
\end{figure}

\begin{figure}[h] 
    \centering
    \includegraphics[width=0.5\textwidth]{logo_um_2022_rouge_RVB.png} 
\end{figure}

\newpage

\setcounter{section}{-1}
\tableofcontents

\newpage
\section{Préambule}
Pour les calculs, nous avons choisi de faire l'exercice 1 (\ref{Exercice1}) en utilisant le logarithme en base 2, et les exercices 2 et 3 (\ref{exercice2} et \ref{exercice3}) en logarithme népérien.\\
En effet, calculer avec le logarithme népérien ou avec le logarithme en base 2 revient au même. Les deux logarithmes sont les mêmes à un facteur près.


\[
\log_2(x) = \frac{\ln(x)}{\ln(2)}
\]


\section{Exercice 1}
\label{Exercice1}

On considère le tableau ci-dessous, répartissant la population active occupée selon l'âge ($A$), le sexe ($S$) et la
catégorie socioprofessionnelle ($C$) (source: INSEE, enquête emploi 2016).
\begin{figure}[ht]
  \centering
  \setlength{\fboxsep}{0pt}  % Pour que l'image soit bien ajustée dans le cadre
  \setlength{\fboxrule}{1pt}  % Épaisseur du cadre
  \fbox{%
    \includegraphics[width=0.7\textwidth]{CE.png}
  }
  \caption{Tableau répartissant la population active occupée selon des catégories }
\end{figure}
\subsection{Modélisation et variable}
\label{1.1}
Tout d'abord de manière intuitive, nous avons envie de modéliser la variable
socioprofessionnelle avec les deux autres. Cependant, nous devons
le montrer de manière formelle. Grâce au code fourni dans la partie \ref{sec : Annexe},
nous calculons l'information mutuelle de chacune des variables.\\\\
Premièrement, calculons l'entropie de chacune de ces variables. Pour la variable $A$, nous avons le tableau suivant (en fréquence). 
\begin{table}[ht]
  \centering
  \caption{Distribution par âge ($A$)}
  \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    & 15-29 ans  & 30-39 ans  & 40-49 ans  & 50-59 ans  & 60+ ans \\ \hline
    Age & 0.1866 & 0.2419 & 0.2730 & 0.2441 & 0.0542 \\ \hline
  \end{tabular}
\end{table}
\\
Nous pouvons calculer l'entropie de $A$.\\
\[
H(A) = -\sum_{n = 1}^{5}p_i\log_2(p_i)=2,1833
\]
De la même manière, nous calculons l'entropie de $C$ et $S$. Nous utilisons les tableaux ci-dessous.\\
\begin{table}[H]
  \centering
  \caption{Distribution par sexe ($S$)}
  \begin{tabular}{|l|l|l|}
  \hline
             & Femme  & Homme  \\ \hline
  Proportion & 0,6589 & 0,3411 \\ \hline
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Distribution par catégorie socioprofessionnelle ($C$)}
  \begin{tabular}{|l|l|l|l|l|l|l|}
  \hline
             & Agriculteur & Artisans & Cadres & Profession In & Employes & Ouvrier \\ \hline
  Proportion & 0,0207      & 0,0740   & 0,1875 & 0,2512        & 0,2240   & 0,2423  \\ \hline
  \end{tabular}
  \end{table}


Nous obtenons.
\[
H(S) = 0,9258  \hspace{2 cm} H(C)= 2,3266
\]
A cette étape, nous pouvons interpréter les résulats. L'entropie d'une variable, mesure l'incertitude liée a celle-ci. Donc la variable $C$ est la variable avec l'incertitude la plus élevée.\\
Nous allons maintenant calculer les valeurs suivantes : $H(A,S)$, $H(A,C)$ et $H(S,C)$. Pour ce faire, nous déterminerons les tableaux joints correspondants pour chacune de ces paires de variables. Puis nous calculerons l'entropie de ces paires.
\[
  H(X) = - \sum_{x \in Tableau} P(x) \log_2(P(x))
\]
Nous trouvons.
\begin{table}[H]
  \centering
  \caption{Distribution jointe sexe ($S$) et âge ($A$)}
  \begin{tabular}{|l|l|l|l|l|l|}
  \hline
        & 15-29 ans & 30-39 ans & 40-49 ans & 50-59 ans & 60+    \\ \hline
  Femme & 0,1220    & 0,1584    & 0,1803    & 0,1618    & 0,0362 \\ \hline
  Homme & 0,0645    & 0,0834    & 0,0927    & 0,0823    & 0,1802 \\ \hline
  \end{tabular}
\end{table}

  \begin{table}[H]
    \centering
    \caption{Distribution jointe ($C$) et âge ($A$)}
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
                   & 15-29 ans & 30-39 ans & 40-49 ans & 50-59 ans & 60+    \\ \hline
    Agriculteur    & 0,0013    & 0,0031    & 0,0052    & 0,0081    & 0,0030 \\ \hline
    Artisans       & 0,0049    & 0,0153    & 0,0234    & 0,0225    & 0,0080 \\ \hline
    Cadres         & 0,0219    & 0,0471    & 0,0568    & 0,0468    & 0,0149 \\ \hline
    Professions In & 0,0489    & 0,0665    & 0,0699    & 0,0563    & 0,0094 \\ \hline
    Employes       & 0,0509    & 0,0511    & 0,0571    & 0,0535    & 0,0113 \\ \hline
    Ouvrier        & 0,0586    & 0,0586    & 0,0604    & 0,0569    & 0,0077 \\ \hline
    \end{tabular}
  \end{table}

\begin{table}[H]
  \centering
  \caption{Distribution jointe ($C$) et ($S$)}
  \begin{tabular}{|l|l|l|l|l|l|l|}
  \hline
         & Agriculteur & Artisans & Cadres & Profession IN & Employes & Ouvrier \\ \hline
    Femmes & 0.0119      & 0.0433   & 0,1175 & 0.1705        & 0.1810   & 0.1344  \\ \hline
    Homme  & 0.0087      & 0.0307   & 0,0700 & 0.0807        & 0.0430   & 0.1079  \\ \hline
    \end{tabular}
\end{table}
Nous obtenons les valeurs suivantes.
\[
H(A,S)=3,1092 \hspace{1 cm} H(A,C)= 4,4817 \hspace{1cm} H(C,S)=3,2242
\]
De plus, nous obtenons pour $H(A,S,C)$ la valeur suivante.
\[
  H(A,C,S) = -\sum_{n = 1}^{72}p_i\log_2(p_i)=5,3778
\]
Nous pouvons calculer les informations mutuelles.
\[
I(C,(AS))=H(C)+H(A,S)-H(A,S,C) = 0,0580 \\
\]
\[
I(A,(SC))= 0,029803
\]
\[
I(S,(CA))=0,029813
\]
L’information mutuelle mesure la quantité d’information partagée entre deux variables aléatoires, indiquant dans quelle mesure la connaissance de l’une des variables réduit l’incertitude concernant l’autre. 
Avec les calculs précédents, nous observons que la connaissance de la catégorie socioprofessionelle réduit le plus l'incertitude sur l'âge et le sexe.\\
De plus, une information mutuelle proche de 0 suggère une indépendance.\\
Calculons le rapport entre l'information mutuelle et la variable conditionnée le plus élevé.
\[
R_1 = \frac{I(C,(AS))}{H(A,C)}=0,0187
\]
\[
R_2 = \frac{I(A,(SC)))}{H(S,C)}=0,0092
\]
\[
R_3 = \frac{I(S,(CA)))}{H(A,C)}=0,0066
\]
Le rapport $R_1$ est le plus élevé. Donc la variable $C$ modélisé par les deux autres ($A$ et $S$) nous donne le plus d'information.
En d'autres termes, la connaissance de la catégorie socioprofessionnelle et du couple âge, sexe apporte un gain d'information sur le couple âge, catégorie socioprofessionnelle. Et ce gain est de $1,8\%$.
\subsection{Arbre de segmentation binaire}
Grâce à la partie \ref{1.1}, nous pouvons calculer les informations mutuelles suivantes.
\[
I(A,S) = H(A) + H(S) - H(A,S) = 5,1104*10^{-5}
\]
\[
I(A,C) = 0,02830
\]
\[
I(S,C)= 0,02831
\]
Les résultats ci-dessus, permettent de faire les interprétations suivantes. La variable âge et sexe sont proches de l'indépendance. Donc l'une n'influe presque pas sur l'autre. Et l'information mutuelle entre $A$ et $C$ est très proche de celle entre $S$ et $C$.
\\
Calculons maintenant la somme des informations mutuelles de chaque variable avec les deux autres. En pratique, nous souhaitons identifier la variable qui fournit le plus d'informations sur les autres. Cela revient à calculer :
\begin{equation}
    \bar{I}_k = \sum_{n \neq k} I(X_n, X_k)
    \label{eq : I bar}
\end{equation}
Ensuite, nous choisirons la variable pour laquelle \( \bar{I}_k \) est maximale.

\[
\bar{I_A} = I(A,S) + I(A,C) = 0,02835
\]
\[
\bar{I_S}= 0,02836 
\]
\[
\bar{I_C} = 0,0566
\]
Nous avons donc $\bar{I_C}$ qui est le plus élevé. Ainsi, l'arbre de segmentation commencera avec la variable $C$, car elle fournit le plus d'information sur $A$ et $S$.
\\
Ce résultat était attendu, il correspond avec ce qu'on a trouvé dans la partie \ref{1.1}. Cela nous conforte dans l'idée que la variables $C$ nous apporte plus d'informations sur les deux autres.
\\
De plus, lors de la construction de l'arbre, le choix de la deuxième variable de segmentations n'a pas d'importance, étant donné qu'il nous reste deux variables de segmentation. 
\newpage
\begin{figure}
  \begin{center}
    \begin{tikzpicture}[
      grow=down,
      sibling distance=30mm,
      level distance=20mm, % Distance entre les niveaux
      every node/.style={draw, rounded corners, align=center, fill=white!20}
      ]
    \node {Catégories Socioprofessionnelles}
      child { node {Agriculture \\ (833)} % Effectif fictif pour Agriculture
        child { node {...} } % Points de suspension pour Agriculture
      }
      child { node {Artisans \\ (2974.2)} % Effectif fictif pour Artisans
        child { node {...} } % Points de suspension pour Artisans
      }
      child { node {Cadres \\ (7538.3)} % Effectif fictif pour Cadres
        child { node {Femme \\ (4725)} % Effectif fictif pour Femme
          child { node {Âge} 
            child { node {15 à 29 ans \\ (564.9)} } % Effectif pour Femme
            child { node {30-39 ans \\ (1209)} }
            child { node {40-49 ans \\ (1429.5)} }
            child { node {50-59 ans \\ (1161.6)} }
            child { node {60+ ans \\ (360)} }
          }
        } % Fin du nœud Femme
        child { node {Homme \\ (2813.3)} % Effectif fictif pour Homme
          child { node {...} 
          }
        } % Fin du nœud Homme
      }
      child { node {Professions Intellectuelles \\ (10096.7)} % Effectif fictif pour Professions Intellectuelles
        child { node {...} } % Points de suspension pour Professions Intellectuelles
      }
      child { node {Employés \\ (9003.6)} % Effectif fictif pour Employés
        child { node {...} } % Points de suspension pour Employés
      }
      child { node {Ouvriers \\ (9738.8)} % Effectif fictif pour Ouvriers
        child { node {...} } % Points de suspension pour Ouvriers
      };
    \end{tikzpicture}
    \caption{Arbre de segmentation binaire}
    \end{center}
\end{figure}


\newpage
\section{Exercice 2}
\label{exercice2}
\subsection{Recodages}

\subsubsection{Recodage en 2 variables}

En agrégeant seulement les classes contiguës, nous avons 4 possibilités de regroupement binaire de 
$X$.
\\
 

\[
Z_1 =\{ \{0 \% \} , \{0 - 0.5 \% ,0.5-1 \% ,1-3 \% ,>3 \% \} \}  
\]
\[
Z_2 =\{ \{0 \% , 0 - 0.5 \%  \} , \{0.5-1 \% ,1-3 \% ,>3 \% \} \}
\]
\[
Z_3 =\{ \{0 \%, 0 - 0.5 \% ,0.5-1 \%  \} , \{1-3 \% ,>3 \% \} \}
\]
\[
Z_4 =\{ \{0 \% , 0 - 0.5 \% ,0.5-1 \% ,1-3 \% \} , \{>3 \% \} \}
\]


L'entropie de chacun de ses recodages se calcule numériquement.
\\
Détaillons le premier calcul. Nous cherchons le tableau associé a $Z_1$, puis nous calculons $H(Z_1)$.
\begin{table}[h!]
  \centering
  \caption{Tableau entre $Y$ et $Z_1$}
  \begin{tabular}{|c|c|c|}
  \hline
  $Y \backslash Z_1$ & 0\% & $\{0-0.5\%, 0.5-1\%, 1-3\%, \geq 3\%\}$ \\
  \hline
  O & 2 & 20 \\
  \hline
  N & 27 & 23 \\
  \hline
  \end{tabular}
  \end{table}

\[
H(Z_1) = -\left(\frac{29}{72}ln(\frac{29}{72})+\frac{43}{72}ln(\frac{43}{72})\right) = 0.674
\]
De la même manière, nous trouvons.
\[
H(Z_2) = 0.661 \hspace{1cm} H(Z_3) = 0.427 \hspace{1cm} H(Z_4) = 0.073
\]
Le meilleur recodage est donc le premiers c'est à dire :
$Z1 =\{ \{0 \% \} , \{0 - 0.5 \% ,0.5-1 \% ,1-3 \% ,>3 \% \} \}$.\\
En effet $H(Z_1)$ à l'entropie la plus élevée.


\subsubsection{Recodage en 3 variables}

En procédant de la même façon, on considère alors 6 cas :

\[
Z_1 =\{ \{0 \% \} , \{0 - 0.5 \%\} ,\{0.5-1 \% ,1-3 \% ,>3 \% \} \}  
\]
\[
  Z_2 =\{ \{0 \% \} , \{0 - 0.5\%, 0.5-1 \% \} ,\{1-3 \% ,>3 \% \} \}
\]
\[
  Z_3 =\{ \{0 \% \} , \{0 - 0.5\%, 0.5-1 \% ,1-3 \%  \} ,\{>3 \% \} \}
\]
\[
  Z_4 =\{ \{0 \%, 0 - 0.5\%\} , \{ 0.5-1 \% ,1-3 \%  \} ,\{>3 \% \} \}
\]
\[
  Z_5 =\{ \{0 \%, 0 - 0.5\%\} , \{ 0.5-1 \% \} ,\{1-3 \% , >3 \% \} \}
\]
\[
  Z_6 =\{ \{0 \%, 0 - 0.5\%,  0.5-1 \% \} , \{1-3 \%  \} ,\{>3 \% \} \}
\]
\\
L'entropie est calculée numériquement :
\[
H(Z_1) = 1,068 \hspace{0.5cm} H(Z_2) = 1,014 \hspace{0.5cm} H(Z_3) = 0,740 \hspace{0.5cm} H(Z_4) = 0,721 \hspace{0,5cm} H(Z_5) = 0.915 \hspace{0,5 cm} H(Z_6) = 0.474
\]
Nous remarquons que le meilleur recodage en trois variables est $Z_1$.

\subsection{Le meilleur recodage de X pour prédire Y }

Il s'agit ici de recoder $X$ en réduisant l'incertitude sur $Y$.
Nous devons donc trouver le $Z_k$ qui maximise l'information mutuelle entre $Z_k$ et Y.
\\
C'est à dire, trouvons le recodage $Z_k$ qui maximise :
\[
I(Z_k,Y) = H(Z_k) + H(Y) - H(Z_k,Y).
\]

\underline{Détaillons le calcul pour le premier recodage :}

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
  \hline
      & 0\%   & $\neq$ 0\% \\ \hline
  OUI & 2/72  & 20/72 \\ \hline
  NON & 27/72 & 23/72 \\ \hline
  \end{tabular}
  \end{table}


  On a :
\[
H(Z_1)= -\left(\frac{29}{72}ln(\frac{29}{72})+\frac{43}{72}ln(\frac{43}{72})\right)
\]
\[
H(Y)=-\left(\frac{22}{72}ln(\frac{22}{72})+\frac{50}{72}ln(\frac{50}{72})\right)
\]
\[
H(Z_1,Y)=-\left(\frac{2}{72}ln(\frac{2}{72})+\frac{27}{72}ln(\frac{27}{72})+\frac{20}{72}ln(\frac{20}{72})+\frac{23}{72}ln(\frac{23}{72})\right)
\]


Nous trouvons alors:
\[
  I(Z_1, Y)= 0.102
\]
Faisons le même cheminement, mais avec les autres variables. Nous obtenons alors les informations mutuelles suivante.

\[
I(Z_2,Y) = 0.063
\]

\[
I(Z_3,Y) = 0.023
\]

\[
I(Z_4,Y) = 0.016
\]
On observe que l'information mutuelle la plus élevée est $I(Z_1,Y)$. Donc le recodage avec la variable $Z_1$ est le meilleur permettant de prédire $Y$. En effet, une information mutuelle élevée indique que les deux variables partagent de l'information en commun. De plus lors d'un recodage de variable, choisir l'information mutuelle la plus élevée, nous donne plus d'information sur $Y$.


\newpage
\section{Exercice 3}
\label{exercice3}
\subsection{Description}

On considère le tableau recensant des espèces de champignons suivant 4 variables qualitatifs. 

\begin{table}[h]
  \centering
  \caption{Caractéristiques des espèces de champignons}
  \begin{tabular}{@{}ccccc@{}}
  \toprule
  Espèce & Comestible & Chapeau & Tige & Couleur \\ \midrule
  a      & o          & a       & e    & b       \\
  b      & o          & a       & e    & j       \\
  c      & o          & a       & e    & b       \\
  d      & o          & pl      & f    & j       \\
  e      & o          & pl      & f    & b       \\
  f      & n          & po      & f    & r       \\
  g      & n          & po      & f    & j       \\
  h      & n          & po      & e    & r       \\
  i      & n          & a       & f    & j       \\
  j      & n          & pl      & f    & j       \\ \bottomrule
  \end{tabular}
  
  \label{tab:champignons}
  \end{table}

  On cherche à faire un arbre de discrimination qui permet de prédire la comestibilité à partir des autres caractéristiques, tout en étant le plus court possible.
  \\
  \\
  Pour ce faire, on calcul les informations mutuelles de $X_1$ avec chaque autre variable.
  En effet, nous allons chercher la variable qui informe le plus sur la comestibilité. Nous effectuerons nos calculs avec le logarithme népérien.

Nous posons pour la suite.
\[
X_1 = \{Comestible\} \hspace{1cm} X_2 = \{Chapeau\} \hspace{1cm} X_3 = \{Tige\} \hspace{1cm} X_4 = \{Couleur\}
\]

\subsection{Calculs}

\subsubsection{Premiere étape}
On calcul l'information mutuelle avec la formule suivante, $I(X_1,X_2) = H(X_1) + H(X_2) - H(X_1,X_2)$.

\begin{table}[H]
  \centering
    \caption{Croissement entre comestibilité $(X_1)$ et forme du chapeau $(X_2)$}
    \begin{tabular}{|l|l|l|l|}
    \hline
    X1\textbackslash{}X2 & Po & a & Pl \\ \hline
    O                    & 0  & 3 & 2  \\ \hline
    N                    & 3  & 1 & 1  \\ \hline
    \end{tabular}
\end{table}
Nous trouvons grâce à ce tableau, $I(X_1,X_2)$=$H(X_1)+H(X_2)-H(X_1,X_2)$=0.277.
\\\\
Pour les variables suivantes, nous avons ces tableaux.

\begin{table}[H]
  \centering
    \caption{Croisement entre comestibilité $(X_1)$ et la tige $(X_3)$}
    \begin{tabular}{|l|c|c|}
    \hline
    $X_1 \backslash X_3$ & e & f \\ \hline
    O                    & 3  & 2  \\ \hline
    N                    & 1  & 4  \\ \hline
    \end{tabular}
\end{table}

\begin{table}[H]
  \centering
    \caption{Croisement entre comestibilité $(X_1)$ et la couleur $(X_4)$}
    \begin{tabular}{|l|l|l|l|}
      \hline
      $X_1\backslash X_2$ & b & j & r \\ \hline
      O                    & 3  & 2 & 0  \\ \hline
      N                    & 0  & 3 & 2  \\ \hline
      \end{tabular}
\end{table}

De la même manière, nous pouvons calculer l'information mutuelle et nous trouvons :




\[
  I(X_1,X_3)=0.106 \hspace{1cm} I(X_1,X_4)=0.357
\]




Nous pouvons conclure à cette étape que la variable qui apporte le plus d'information sur la comestibilité du champignon est sa couleur. En effet, l'information mutuelle $I(X_1,X_4)$ est la plus élevée.
\\
De plus, on se rend compte dans le tableau \ref{tab:champignons} que les champignons bruns sont comestibles alors que les rouges ne le sont pas.
\\
Traitons le cas des champignons jaune.
    
\subsubsection{Deuxieme étape : champignons jaune}
En considérant seulement les champignons jaunes, on calcul les informations mutuelles suivant les autres variables ($X_2$ et $X_3$). Nous obtenons les tableaux suivants.

\begin{table}[H]
  \caption{Tableau de $X_1$,$X_2$ avec champignon jaune}
  \centering
  \begin{tabular}{|l|l|l|l|}
  \hline
  $X_1\backslash X_2$ & a & pl & po \\ \hline
  O                    & 1 & 1  & 0  \\ \hline
  N                    & 1 & 1  & 1  \\ \hline
  \end{tabular}
  \end{table}

\begin{table}[H]
  \centering
  \caption{Tableau de $X_1$,$X_3$ avec champignon jaune}
  \begin{tabular}{|l|l|l|}
  \hline
  $X_1\backslash X_3$ & e & f \\ \hline
  O                    & 1 & 1 \\ \hline
  N                    & 0 & 2 \\ \hline
  \end{tabular}
  \end{table}
Nous calculons les informations mutuelles et nous obtenons les résultats suivants.
\[
  I(X_1,X_2)=0.118 \hspace{1cm} I(X_1,X_3)=0.223.
\]

On considère alors la variable $X_3$, celle qui correspond à la tige du champignon. Si le champignon est jaune et que la tige est épaisse, alors il est comestible. Sinon il faut distinguer entre le champignon d (comestible) et les champignons g,i,j.
Mais seuls les champignons d et j partagent la même forme du chapeau (plat), alors que les champignons g et i sont respectivement pointu, arrondi et tout deux non comestibles.
\newpage
\subsection{Arbre de discrimination}

Avec toutes les informations mutuelles calculées, on obtient alors l'arbre suivant :



\begin{figure}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}[
    every node/.style={draw, rectangle, rounded corners, align=center, minimum size=10mm, font=\scriptsize},
    level 1/.style={sibling distance=100mm},
    level 2/.style={sibling distance=70mm},
    level 3/.style={sibling distance=35mm},
    level 4/.style={sibling distance=40mm},
    edge from parent/.style={draw, -{Latex[length=5mm]}, thick},
    level distance=35mm
  ]
  
  \node {Couleur}
    child {node {rouge}
        child {node {f,h} edge from parent node[right] {Non comestible}}
    }
    child {node {jaune}
        child {node {Epaisse} 
            child {node {b} edge from parent node[right] {Comestible}}
        }
        child {node {fine}
            child {node {type chapeau}
                child {node {pointu}
                    child {node {non commestible}}
                }
                child {node {arrondi}
                    child {node {non commestible}}
                }
                child {node {plat}
                    child {node {j}
                        child {node {non commestible}}
                    }
                    child {node {d}
                        child {node {commestible}}
                    }
                }
            }
        }
    }
    child {node {brun}
        child {node {a,c,e} edge from parent node[right] {Comestible}}
    };
  \end{tikzpicture}
  }
\caption{Arbre de discrimination pour la comestibilité d'un champignon}
\end{figure}



\newpage
\section{ANNEXE}
\label{sec : Annexe}

\lstset{literate=%
 {é}{{\'e}}1
 {è}{{\`e}}1
 {à}{{\`a}}1
 {ù}{{\`u}}1
}
\lstinputlisting{./main.R}


\end{document}
